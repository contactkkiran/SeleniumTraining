19/12/2015 08:22:43 : INFO  : com.ags.eiq.logging.Log4J : Successfully loaded log4j properties file 
19/12/2015 08:22:43 : INFO  : com.ags.eiq.configuration.ConfigProperties : Fetching the value from the Configuration file with key 'orFilepath' and the result is './objectrepository/ObjectRepository.xml' 
19/12/2015 08:22:43 : INFO  : com.ags.eiq.utils.XMLReader : Started reading of the xml file: ./objectrepository/ObjectRepository.xml 
19/12/2015 08:22:43 : INFO  : com.ags.eiq.utils.XMLReader : Completed reading of the xml file: ./objectrepository/ObjectRepository.xml 
19/12/2015 08:22:45 : INFO  : org.apache.spark.SparkContext : Running Spark version 1.5.0 
19/12/2015 08:22:46 : WARN  : org.apache.hadoop.util.NativeCodeLoader : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 
19/12/2015 08:22:46 : INFO  : org.apache.spark.SecurityManager : Changing view acls to: kkanumuri 
19/12/2015 08:22:46 : INFO  : org.apache.spark.SecurityManager : Changing modify acls to: kkanumuri 
19/12/2015 08:22:46 : INFO  : org.apache.spark.SecurityManager : SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(kkanumuri); users with modify permissions: Set(kkanumuri) 
19/12/2015 08:22:48 : INFO  : akka.event.slf4j.Slf4jLogger : Slf4jLogger started 
19/12/2015 08:22:48 : INFO  : Remoting : Starting remoting 
19/12/2015 08:22:48 : INFO  : Remoting : Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.16.0.8:56804] 
19/12/2015 08:22:48 : INFO  : org.apache.spark.util.Utils : Successfully started service 'sparkDriver' on port 56804. 
19/12/2015 08:22:48 : INFO  : org.apache.spark.SparkEnv : Registering MapOutputTracker 
19/12/2015 08:22:48 : INFO  : org.apache.spark.SparkEnv : Registering BlockManagerMaster 
19/12/2015 08:22:49 : INFO  : org.apache.spark.storage.DiskBlockManager : Created local directory at C:\Users\kkanumuri\AppData\Local\Temp\blockmgr-fd600e96-169c-43d3-8e5a-142e48663474 
19/12/2015 08:22:49 : INFO  : org.apache.spark.storage.MemoryStore : MemoryStore started with capacity 972.5 MB 
19/12/2015 08:22:49 : INFO  : org.apache.spark.HttpFileServer : HTTP File server directory is C:\Users\kkanumuri\AppData\Local\Temp\spark-2b69050c-f0fe-478b-aec0-3c3cea85c84f\httpd-d3b86890-a5a2-48e3-98a3-6bc18aa4d767 
19/12/2015 08:22:49 : INFO  : org.apache.spark.HttpServer : Starting HTTP Server 
19/12/2015 08:22:49 : INFO  : org.spark-project.jetty.server.Server : jetty-8.y.z-SNAPSHOT 
19/12/2015 08:22:49 : INFO  : org.spark-project.jetty.server.AbstractConnector : Started SocketConnector@0.0.0.0:56805 
19/12/2015 08:22:49 : INFO  : org.apache.spark.util.Utils : Successfully started service 'HTTP file server' on port 56805. 
19/12/2015 08:22:49 : INFO  : org.apache.spark.SparkEnv : Registering OutputCommitCoordinator 
19/12/2015 08:22:49 : INFO  : org.spark-project.jetty.server.Server : jetty-8.y.z-SNAPSHOT 
19/12/2015 08:22:49 : INFO  : org.spark-project.jetty.server.AbstractConnector : Started SelectChannelConnector@0.0.0.0:4040 
19/12/2015 08:22:49 : INFO  : org.apache.spark.util.Utils : Successfully started service 'SparkUI' on port 4040. 
19/12/2015 08:22:49 : INFO  : org.apache.spark.ui.SparkUI : Started SparkUI at http://172.16.0.8:4040 
19/12/2015 08:22:50 : INFO  : org.apache.spark.scheduler.FairSchedulableBuilder : Created default pool default, schedulingMode: FIFO, minShare: 0, weight: 1 
19/12/2015 08:22:50 : WARN  : org.apache.spark.metrics.MetricsSystem : Using default name DAGScheduler for source because spark.app.id is not set. 
19/12/2015 08:22:50 : INFO  : org.apache.spark.deploy.client.AppClient$ClientEndpoint : Connecting to master spark://10.0.16.16:7077... 
19/12/2015 08:23:10 : ERROR : org.apache.spark.util.SparkUncaughtExceptionHandler : Uncaught exception in thread Thread[appclient-registration-retry-thread,5,main] 
java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.FutureTask@b29a18a rejected from java.util.concurrent.ThreadPoolExecutor@5a2c70bf[Running, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 0]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372)
	at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:110)
	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$tryRegisterAllMasters$1.apply(AppClient.scala:96)
	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$tryRegisterAllMasters$1.apply(AppClient.scala:95)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
	at org.apache.spark.deploy.client.AppClient$ClientEndpoint.tryRegisterAllMasters(AppClient.scala:95)
	at org.apache.spark.deploy.client.AppClient$ClientEndpoint.org$apache$spark$deploy$client$AppClient$ClientEndpoint$$registerWithMaster(AppClient.scala:121)
	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$2$$anonfun$run$1.apply$mcV$sp(AppClient.scala:132)
	at org.apache.spark.util.Utils$.tryOrExit(Utils.scala:1119)
	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$2.run(AppClient.scala:124)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
19/12/2015 08:23:10 : INFO  : org.apache.spark.storage.DiskBlockManager : Shutdown hook called 
19/12/2015 08:23:10 : INFO  : org.apache.spark.util.ShutdownHookManager : Shutdown hook called 
19/12/2015 08:23:10 : INFO  : org.apache.spark.util.ShutdownHookManager : Deleting directory C:\Users\kkanumuri\AppData\Local\Temp\spark-2b69050c-f0fe-478b-aec0-3c3cea85c84f\httpd-d3b86890-a5a2-48e3-98a3-6bc18aa4d767 
19/12/2015 08:23:10 : INFO  : org.apache.spark.util.ShutdownHookManager : Deleting directory C:\Users\kkanumuri\AppData\Local\Temp\spark-2b69050c-f0fe-478b-aec0-3c3cea85c84f\userFiles-ffbce619-1310-495b-9872-55f36740f021 
19/12/2015 08:23:10 : INFO  : org.apache.spark.util.ShutdownHookManager : Deleting directory C:\Users\kkanumuri\AppData\Local\Temp\spark-2b69050c-f0fe-478b-aec0-3c3cea85c84f 
